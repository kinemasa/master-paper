import os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import ctypes
import re
from typing import Optional
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from scipy.signal import butter, filtfilt, detrend
from typing import List, Tuple, Optional
from scipy.signal import resample_poly
from pathlib import Path
from typing import Optional, Iterable
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import glob
import datetime
from typing import Optional, List, Tuple, Dict, Iterable
## ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ»ãƒ­ãƒ¼ãƒ‰ç³»
from myutils.select_folder import select_folder
from myutils.load_and_save_folder import load_ppg_pulse
from deep_learning.evaluation import total_loss,weighted_mae,mae,mae_and_corr
from deep_learning.lstm import ReconstractPPG_with_QaulityHead
from pulsewave.processing_pulsewave import detrend_pulse,bandpass_filter_pulse
import json
import tkinter as tk
from tkinter import messagebox
import csv
def ask_yes_no(msg: str) -> bool:
    root = tk.Tk()
    root.withdraw()
    return messagebox.askyesno("ç¢ºèª", msg)

_NUM_RE = re.compile(r"(\d+)")  # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æœ€åˆã®æ•°å­—åˆ—ã‚’æ‹¾ã†



CONFIG_PATH = Path("./last_folders.json")

def ask_yes_no(msg: str) -> bool:
    root = tk.Tk()
    root.withdraw()
    return messagebox.askyesno("ç¢ºèª", msg)

def _pick_or_load_folders():
    """
    å‰å›é¸ã‚“ã ãƒ•ã‚©ãƒ«ãƒ€ã‚’å†åˆ©ç”¨ã™ã‚‹ã‹å°‹ã­ã‚‹ã€‚
    å†åˆ©ç”¨ãªã‚‰JSONã‚’ãƒ­ãƒ¼ãƒ‰ã€æ–°è¦ãªã‚‰select_folder()ã‚’ä½¿ã£ã¦å†è¨­å®šã€‚
    """
    folder_labels = ["ICA", "POS", "CHROM", "LGI", "OMIT", "PPGï¼ˆROIãªã—/phaseå›ºå®šï¼‰"]

    if CONFIG_PATH.exists() and ask_yes_no("å‰å›ã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ"):
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            saved = json.load(f)
        print("âœ… å‰å›ã®ãƒ•ã‚©ãƒ«ãƒ€è¨­å®šã‚’å†åˆ©ç”¨ã—ã¾ã™ã€‚")
        return {label: Path(saved[label]) for label in folder_labels}

    # æ–°è¦é¸æŠ
    folders = {}
    for label in folder_labels:
        p = select_folder(message=f"{label} ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã—ã¦ãã ã•ã„")
        if not p:
            raise RuntimeError(f"{label} ã®ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
        folders[label] = Path(p)

    # ä¿å­˜
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        json.dump({k: str(v) for k, v in folders.items()}, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ é¸æŠã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {CONFIG_PATH}")

    return folders

def _build_index(folder: Path, suffixes: Iterable[str]=(".csv",)) -> Dict[int, Path]:
    """
    æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {sid(int): Path} ã‚’ä½œã‚‹ã€‚
    ãƒ«ãƒ¼ãƒ«: ãƒ•ã‚¡ã‚¤ãƒ«åä¸­ã®ã€Œæœ€åˆã®æ•°å­—åˆ—ã€ã‚’ sid ã¨ã¿ãªã™ï¼ˆ001.csv / sub001.txt ãªã©ï¼‰ã€‚
    """
    idx: Dict[int, Path] = {}
    for suf in suffixes:
        for p in folder.glob(f"*{suf}"):
            m = _NUM_RE.search(p.stem)
            if not m:
                continue
            sid = int(m.group(1))
            # æ—¢ã«ã‚ã‚Œã°å¾Œå‹ã¡ãƒ»å…ˆå‹ã¡ã¯å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´å¯
            idx[sid] = p
    return idx


def load_pulse(filepath):
    """time_sec, pulse ã‚’æŒã¤CSV/TXTã‚’èª­ã¿è¾¼ã‚“ã§DataFrameã‚’è¿”ã™"""
    try:
        # åŒºåˆ‡ã‚Šè‡ªå‹•åˆ¤å®š (ã‚«ãƒ³ãƒ/ã‚¿ãƒ–/ã‚¹ãƒšãƒ¼ã‚¹å¯¾å¿œ)
        df = pd.read_csv(filepath, sep=None, engine="python")
        # åˆ—åã‚’å°æ–‡å­—åŒ–ã—ã¦å¯¾å¿œ
        cols_lower = {c.lower(): c for c in df.columns}
        # if "time_sec" not in cols_lower or "pulse" not in cols_lower:
        #     raise ValueError(f"'time_sec' ã¾ãŸã¯ 'pulse' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {df.columns}")

        # æ•°å€¤åŒ–ï¼ˆæ–‡å­—æ··å…¥ã¯ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ï¼‰
        df["time_sec"] = pd.to_numeric(df[cols_lower["time_sec"]], errors="raise")
        df["value"] = pd.to_numeric(df[cols_lower["value"]], errors="raise")

        return df

    except Exception as e:
        print(f"[load_pulse_csv] èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def preprocess_ppg_signal(ppg_signal: np.ndarray, fs_ppg: int = 100, fs_target: int = 30) -> np.ndarray:
    """
    PPGä¿¡å·ã‚’ rPPG ã¨åŒã˜å‡¦ç†ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«åˆã‚ã›ã‚‹ã€‚
    1) 100Hz â†’ 30Hz ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    2) ãƒ‡ãƒˆãƒ¬ãƒ³ãƒ‰
    3) ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ (0.7â€“3Hz)
    4) Zã‚¹ã‚³ã‚¢æ­£è¦åŒ–
    """

    # --- (1) 100Hz â†’ 30Hz ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° ---
    ppg_ds = resample_poly(ppg_signal, up=3, down=10, window=('kaiser', 5.0))

    # --- (2) ãƒ‡ãƒˆãƒ¬ãƒ³ãƒ‰ ---
    try:
        ppg_dt = detrend_pulse(ppg_ds, sample_rate=fs_target)
    except Exception:
        # é«˜åº¦ãªdetrendãŒå¤±æ•—ã™ã‚‹å ´åˆã¯numpyç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from scipy.signal import detrend
        ppg_dt = detrend(ppg_ds)

    # --- (3) ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ (0.7â€“3Hz) ---
    ppg_bp = bandpass_filter_pulse(ppg_dt, band_width=[0.1, 10.0], sample_rate=fs_target)
    ##ã€€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®PPGã¯åè»¢ã•ã›ãŸã»ã†ãŒè‰¯ã„ã®ã§é€†ã§èª­ã¿è¾¼ã‚€
    ppg_bp = - ppg_bp 


    return ppg_bp.astype(np.float32)

def resample_ppg_100_to_30(ppg_signal: np.ndarray) -> np.ndarray:
    """
    100 Hz ã® PPG ä¿¡å·ã‚’ 30 Hz ã«å¤‰æ›ã™ã‚‹ï¼ˆFIR ä½åŸŸãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼‰
    ä½ç›¸æ­ªã¿ãŒå°‘ãªãã€ã‚¨ã‚¤ãƒªã‚¢ã‚·ãƒ³ã‚°ã‚‚æŠ‘åˆ¶ã•ã‚Œã‚‹ã€‚
    """
    # 100 Hz â†’ 30 Hz ãªã®ã§ up=3, down=10
    y30 = resample_poly(ppg_signal, up=3, down=10, window=('kaiser', 5.0))
    return y30.astype(np.float32)


def windowize(X: np.ndarray, y: np.ndarray, fs: int, win_sec: int, hop_sec: int):
    win = win_sec * fs
    hop = hop_sec * fs
    T = len(y)
    out = []
    for start in range(0, T - win + 1, hop):
        xs = X[start:start+win, :].astype(np.float32)  # (win, C)
        ys = y[start:start+win].astype(np.float32)     # (win,)
        # çª“å†…z-scoreï¼ˆå„ãƒãƒ£ãƒãƒ« & æ•™å¸«ï¼‰
        xs = (xs - xs.mean(axis=0)) / (xs.std(axis=0) + 1e-8)
        ys = (ys - ys.mean()) / (ys.std() + 1e-8)
        out.append((xs, ys[:, None]))  # (win,C), (win,1)
    return out


@torch.no_grad()
def export_all_predictions(model, loader, device, fs, out_dir: Path, subset_name: str):
    """
    ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆtrain / val / testï¼‰ã§
    TruePPG / PredPPG / Quality / å„å…¥åŠ›ãƒãƒ£ãƒãƒ«(LGI, POS, CHROM, ICA, OMIT)
    ã‚’CSVã§ä¿å­˜ã™ã‚‹ã€‚ï¼ˆãƒãƒ£ãƒãƒ«ã¯windowizeå¾Œã®z-scoreæ¸ˆã¿å€¤ï¼‰
    """
    model.eval()
    out_dir = out_dir / subset_name
    out_dir.mkdir(parents=True, exist_ok=True)

    sample_idx = 0
    for xs, ys in loader:
        xs = xs.to(device)                      # (B, T, C=5)
        ys = ys.to(device)                      # (B, T, 1)
        y_hat, w_hat, _ = model(xs)             # (B, T, 1), (B, T, 1)

        B, T, _ = y_hat.shape
        for i in range(B):
            sample_idx += 1

            # æ•™å¸«ãƒ»äºˆæ¸¬ãƒ»å“è³ª
            y_true = ys[i, :, 0].cpu().numpy()
            y_pred = y_hat[i, :, 0].cpu().numpy()
            w      = w_hat[i, :, 0].cpu().numpy()

            # å…¥åŠ›5ãƒãƒ£ãƒãƒ«ï¼ˆé †åº: LGI, POS, CHROM, ICA, OMITï¼‰
            X_win = xs[i].cpu().numpy()         # (T, 5)
            lgi   = X_win[:, 0]
            pos   = X_win[:, 1]
            chrom = X_win[:, 2]
            ica   = X_win[:, 3]
            omit  = X_win[:, 4]

            t = np.arange(T) / fs

            df_out = pd.DataFrame({
                "time_sec": t,
                "true_ppg": y_true,
                "pred_ppg": y_pred,
                "quality": w,
                "lgi": lgi,
                "pos": pos,
                "chrom": chrom,
                "ica": ica,
                "omit": omit,
            })

            csv_path = out_dir / f"sample_{sample_idx:05d}.csv"
            df_out.to_csv(csv_path, index=False)

    print(f"âœ… {subset_name} set: {sample_idx} samples exported to {out_dir}")

# ================ Dataset ================
class RppgPpgDataset(Dataset):
    """
    èµ·å‹•æ™‚ã«GUIã§ 6 ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆICA / POS / CHROM / LGI / OMIT / PPGï¼‰ã‚’é¸æŠã€‚
    å„ãƒ•ã‚©ãƒ«ãƒ€ç›´ä¸‹ã« subject_num ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: 001.csvï¼‰ãŒã‚ã‚‹å‰æã€‚

    - åˆå›ã«å„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ glob ã—ã¦ {sid: path} ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆï¼ˆé«˜é€Ÿï¼‰
    - 6ãƒ•ã‚©ãƒ«ãƒ€å…±é€šã§å­˜åœ¨ã™ã‚‹ subject ã®ã¿å­¦ç¿’å¯¾è±¡
    - subj_start / subj_end / omit_ids ã§è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿
    - PPGã¯ ROI ãªã—å‰æï¼ˆPPG_before ç­‰ã®å›ºå®šãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸ã‚“ã§ãã ã•ã„ï¼‰
    """
    def __init__(
        self,
        *,
        fs: int,
        win_sec: int,
        hop_sec: int,
        subj_start: Optional[int] = None,
        subj_end: Optional[int] = None,
        omit_ids: Optional[List[int]] = None,
        allow_txt: bool = False,  # True ã§ .txt ã‚‚å¯¾è±¡ã«
        fs_ppg_src: int = 100,    # å…ƒPPGã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°
    ):
        self.samples: List[Tuple[np.ndarray, np.ndarray]] = []
        self.fs = fs
        self.win_sec = win_sec
        self.hop_sec = hop_sec
        self.fs_ppg_src = fs_ppg_src

        # ---- å¿…ãšGUIã§ãƒ•ã‚©ãƒ«ãƒ€é¸æŠ ----
        folders = _pick_or_load_folders()
        ICA_dir   = folders["ICA"]
        POS_dir   = folders["POS"]
        CHROM_dir = folders["CHROM"]
        LGI_dir   = folders["LGI"]
        OMIT_dir  = folders["OMIT"]
        PPG_dir   = folders["PPGï¼ˆROIãªã—/phaseå›ºå®šï¼‰"]

        # ---- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆå„ãƒ•ã‚©ãƒ«ãƒ€ä¸€åº¦ã ã‘ã‚¹ã‚­ãƒ£ãƒ³ï¼‰----
        suffixes = (".csv", ".txt") if allow_txt else (".csv",)
        idx_ica   = _build_index(ICA_dir,   suffixes)
        idx_pos   = _build_index(POS_dir,   suffixes)
        idx_chrom = _build_index(CHROM_dir, suffixes)
        idx_lgi   = _build_index(LGI_dir,   suffixes)
        idx_omit  = _build_index(OMIT_dir,  suffixes)
        idx_ppg   = _build_index(PPG_dir,   suffixes)

        # ---- å…±é€šIDã‚’æŠ½å‡º ----
        common_ids = (
            set(idx_ica) & set(idx_pos) & set(idx_chrom) &
            set(idx_lgi) & set(idx_omit) & set(idx_ppg)
        )

        # è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆç¯„å›²ãƒ»é™¤å¤–ï¼‰
        if subj_start is not None and subj_end is not None:
            common_ids &= set(range(subj_start, subj_end + 1))
        if omit_ids:
            common_ids -= set(omit_ids)

        if not common_ids:
            raise RuntimeError("å…±é€šã®è¢«é¨“è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é¸ã‚“ã ãƒ•ã‚©ãƒ«ãƒ€ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆsubjectç•ªå·ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ---- èª­ã¿è¾¼ã¿ãƒ«ãƒ¼ãƒ— ----
        for sid in sorted(common_ids):
            p_ica, p_pos, p_chrom = idx_ica[sid], idx_pos[sid], idx_chrom[sid]
            p_lgi, p_omit, p_ppg  = idx_lgi[sid], idx_omit[sid], idx_ppg[sid]

            # èª­ã¿è¾¼ã¿
            df_ica   = load_pulse(p_ica)
            df_pos   = load_pulse(p_pos)
            df_chrom = load_pulse(p_chrom)
            df_lgi   = load_pulse(p_lgi)
            df_omit  = load_pulse(p_omit)
            df_ppg   = load_ppg_pulse(p_ppg)

            if any(d is None for d in [df_ica, df_pos, df_chrom, df_lgi, df_omit, df_ppg]):
                print(f"âš ï¸ skip SID {sid:03d} - èª­ã¿è¾¼ã¿å¤±æ•—ã‚ã‚Š")
                continue

            try:
                s_ica   = df_ica["value"].to_numpy(dtype=float)
                s_pos   = df_pos["value"].to_numpy(dtype=float)
                s_chrom = df_chrom["value"].to_numpy(dtype=float)
                s_lgi   = df_lgi["value"].to_numpy(dtype=float)
                s_omit  = df_omit["value"].to_numpy(dtype=float)
                s_ppg   = df_ppg["value"].to_numpy(dtype=float)
            except KeyError:
                print(f"âš ï¸ skip SID {sid:03d} - 'value' åˆ—ãŒä¸è¶³")
                continue

            # PPGã®ã¿ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼†å‰å‡¦ç†ï¼ˆé–¢æ•°å†…ã§ãƒ‡ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ã‚‚ï¼‰
            s_ppg = preprocess_ppg_signal(s_ppg, fs_ppg=self.fs_ppg_src, fs_target=self.fs)

            # é•·ã•åˆã‚ã›ï¼ˆæœ€çŸ­ï¼‰
            T = min(map(len, [s_ica, s_pos, s_chrom, s_lgi, s_omit, s_ppg]))
            s_ica, s_pos, s_chrom, s_lgi, s_omit, s_ppg = (
                s_ica[:T], s_pos[:T], s_chrom[:T], s_lgi[:T], s_omit[:T], s_ppg[:T]
            )

            # å…¥åŠ›5chï¼ˆé †åºå›ºå®šï¼‰
            X = np.stack([s_lgi, s_pos, s_chrom, s_ica, s_omit], axis=1)  # (T,5)
            y = s_ppg  # (T,)

            self.samples.extend(windowize(X, y, self.fs, self.win_sec, self.hop_sec))

        if len(self.samples) == 0:
            raise RuntimeError("ã‚µãƒ³ãƒ—ãƒ«ãŒ0ä»¶ã§ã—ãŸã€‚IDå¯¾å¿œã‚„çª“è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        xs, ys = self.samples[idx]
        return torch.from_numpy(xs), torch.from_numpy(ys)

# ================ å­¦ç¿’ãƒ˜ãƒ«ãƒ‘ ================
def make_loaders(dataset: RppgPpgDataset, batch_size: int, num_workers: int,
                 train_ratio: float, val_ratio: float):
    N = len(dataset)
    n_train = int(N * train_ratio)
    n_val   = int(N * val_ratio)
    n_test  = N - n_train - n_val
    train_set, val_set, test_set = random_split(dataset, [n_train, n_val, n_test],
                                                generator=torch.Generator().manual_seed(42))
    dl_train = DataLoader(train_set, batch_size=batch_size, shuffle=True,
                          num_workers=num_workers, pin_memory=True, drop_last=False)
    dl_val   = DataLoader(val_set,   batch_size=batch_size, shuffle=False,
                          num_workers=num_workers, pin_memory=True, drop_last=False)
    dl_test  = DataLoader(test_set,  batch_size=batch_size, shuffle=False,
                          num_workers=num_workers, pin_memory=True, drop_last=False)
    return dl_train, dl_val, dl_test

def train_one_epoch(model, loader, optimizer, device):
    model.train()
    total = 0.0
    for xs, ys in loader:
        xs = xs.to(device)          # (B,T,C)
        ys = ys.to(device)          # (B,T,1)
        y_hat, w_hat, _ = model(xs) # (B,T,1), (B,T,1)
        # loss = total_loss(y_hat, ys, w_hat, lam_corr=0.3, lam_cov=0.1, lam_tv=0.01)
        # loss = mae(y_hat, ys,w_hat)
        loss =mae_and_corr(y_hat,ys,1e-8)
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total += loss.item() * xs.size(0)
    return total / len(loader.dataset)

@torch.no_grad()
def evaluate(model, loader, device):
    model.eval()
    total = 0.0
    for xs, ys in loader:
        xs = xs.to(device)
        ys = ys.to(device)
        y_hat, w_hat, _ = model(xs)
        loss = total_loss(y_hat, ys, w_hat, lam_corr=0.3, lam_cov=0.1, lam_tv=0.01)
        total += loss.item() * xs.size(0)
    return total / len(loader.dataset)

    
# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† ======
def main():
    
    # --- ã‚¹ãƒªãƒ¼ãƒ—é˜²æ­¢ã‚’æœ‰åŠ¹åŒ– ---
    # ES_CONTINUOUS | ES_SYSTEM_REQUIRED
    ctypes.windll.kernel32.SetThreadExecutionState(0x80000002)
    
      # ãƒ­ã‚°CSVã®æº–å‚™ï¼ˆExcelã§é–‹ãã‚„ã™ã„ã‚ˆã†ã«UTF-8 BOMä»˜ãï¼‰
    out_root = Path("./outputs-noband")
    out_root.mkdir(parents=True, exist_ok=True)
    hist_csv = out_root / "training_log.csv"

    # åˆå›ãƒ˜ãƒƒãƒ€ï¼ˆæ—¢ã«å­˜åœ¨ã—ã¦ã„ã¦ã‚‚å†ä½œæˆã—ãŸã„å ´åˆã¯ unlink ã—ã¦ã­ï¼‰
    if not hist_csv.exists():
        with open(hist_csv, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["timestamp", "epoch", "train_loss", "val_loss", "lr"])
    
    # ICA_folder = select_folder(message="ICA")
    # POS_folder = select_folder(message="POS")
    # CHROM_folder = select_folder(message="CHROM")
    # LGI_folder = select_folder(message="LGI")
    # OMIT_folder =select_folder(message="OMIT")
    # PPG_folder = select_folder(message="PPG")
    

    train_ratio = 0.7
    val_ratio   = 0.15
    batch_size  = 32
    num_workers = 2
    max_epochs  = 200
    lr          = 1e-3
    device      = "cuda" if torch.cuda.is_available() else "cpu"


    # --- Dataset æ§‹ç¯‰ ---
    dataset = RppgPpgDataset(
    fs=30,            # å­¦ç¿’/æ¨è«–ã§ä½¿ã†ã‚¿ãƒ¼ã‚²ãƒƒãƒˆFs
    win_sec=10,
    hop_sec=10,
    subj_start=1000,     # ä»»æ„
    subj_end=10000,     # ä»»æ„
    omit_ids=[], # ä»»æ„
    allow_txt=False,  # .txtã‚‚å¯¾è±¡ãªã‚‰ True
    fs_ppg_src=100,   # å…ƒPPGã®Fsã«åˆã‚ã›ã¦
    )

    dl_train, dl_val, dl_test = make_loaders(dataset, batch_size, num_workers, train_ratio, val_ratio)

    # --- ãƒ¢ãƒ‡ãƒ« ---
    model = ReconstractPPG_with_QaulityHead(
        input_size=5, lstm_dims=(90,60,30), cnn_hidden=32,
        drop=0.2, combine_quality_with_head=False
    ).to(device)
    # --- æœ€é©åŒ– ---
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)
    print(f"Dataset total: {len(dataset)}")
    print(f"Train/Val/Test = {len(dl_train.dataset)}, {len(dl_val.dataset)}, {len(dl_test.dataset)}")
    # --- å­¦ç¿’ ---
    best_val = float("inf"); best_state = None
    for epoch in range(1, max_epochs+1):
        tr = train_one_epoch(model, dl_train, optimizer, device)
        va = evaluate(model, dl_val, device)
        scheduler.step(va)
        print(f"[{epoch:03d}] train={tr:.4f}  val={va:.4f}  lr={optimizer.param_groups[0]['lr']:.2e}")
        
        # â† ã“ã“ã§CSVã«1è¡Œè¿½è¨˜
        with open(hist_csv, "a", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(f"[{epoch:03d}] train={tr:.4f}  val={va:.4f}  lr={optimizer.param_groups[0]['lr']:.2e}")


        if va < best_val:
            best_val = va
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}

    # --- ãƒ†ã‚¹ãƒˆï¼ˆãƒ™ã‚¹ãƒˆã§ï¼‰ ---
    if best_state is not None:
        model.load_state_dict(best_state)
    te = evaluate(model, dl_test, device)
    print(f"[TEST] loss={te:.4f}")

    # --- ä¿å­˜ ---
    out = Path("./checkpoints"); out.mkdir(parents=True, exist_ok=True)
    save_path = out / "reconppg_quality_best_corrmae.pth"
    torch.save(model.state_dict(), save_path)
    print(f"Saved: {save_path}")
    
    # --- æ¨å®šçµæœã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§æ›¸ãå‡ºã— ---
    out_root = Path("./outputs-corrmae")
    fs=30
    export_all_predictions(model, dl_train, device, fs, out_root, "train")
    export_all_predictions(model, dl_val,   device, fs, out_root, "val")
    export_all_predictions(model, dl_test,  device, fs, out_root, "test")
    
    
    
if __name__ == "__main__":
    main()
